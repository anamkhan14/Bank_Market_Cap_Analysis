{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anamkhan14/Bank_Market_Cap_Analysis/blob/main/ETL_Project_Bank_Data_Analysis_Starter_Anam_khan_doubt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6DKWddiJBDY"
      },
      "source": [
        "# Bank Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMjY9GFCryle"
      },
      "source": [
        "# Objective\n",
        "\n",
        "In this case study, you will be working on Banking Data Analysis using Apache Spark, a powerful distributed computing framework designed for big data processing. This assignment aims to give you hands-on experience in analyzing large-scale banking datasets using PySpark and AWS services. You will apply techniques learned in data analytics to clean, transform, and explore banking data, drawing meaningful insights to support financial decision-making. Apart from understanding how big data tools can optimize performance on a single machine and across clusters, you will develop a structured approach to analyzing market capitalization trends, currency conversions, and global banking performance. Additionally, you will use AWS S3 to store the processed data once the ETL pipeline is complete, ensuring efficient data management and retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DWYojlr1pl"
      },
      "source": [
        "# Business Value\n",
        "\n",
        "The banking industry operates in a highly competitive and globalized market where financial institutions must continuously monitor their market position, performance, and currency fluctuations. To stay competitive, banks must leverage data-driven insights to optimize their financial strategies, assess market trends, and make informed decisions. In this assignment, you will analyze banking data to uncover patterns in market capitalization, currency conversions, and global rankings. With Apache Spark's ability to handle large datasets efficiently, financial institutions can process vast amounts of data in real-time, helping them make faster and more informed decisions.\n",
        "\n",
        "\n",
        "As an analyst at a financial institution, your task is to examine historical banking data to derive actionable insights that can drive strategic growth. Your analysis will help identify the top 10 largest banks by market capitalization, convert market cap values into multiple currencies (USD, GBP, EUR, INR), and store the processed data for easy retrieval. To enhance scalability and accessibility, you will utilize AWS S3 for data storage, ensuring seamless integration with cloud-based analytics tools. By leveraging big data analytics and cloud services, financial institutions can streamline operations, enhance decision-making, and maximize revenue opportunities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEWT70YVr6-9"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The data for this project can be accessed from the following [wikipedia link](https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks).\n",
        "\n",
        "The dataset used in this analysis comprises global banking data extracted from a Wikipedia page (List of Largest Banks) archived as of September 2023.\n",
        "\n",
        "It includes rankings of the world's largest banks based on market capitalization (in USD billions) and is structured into a single table with three columns. The data was scraped programmatically, cleaned, and transformed using PySpark to support cross-currency analysis. Exchange rate data (for USD, GBP, EUR, and INR) was sourced from a CSV file to enable currency conversions. The processed dataset is stored in CSV and SQLite formats for accessibility and efficient querying.\n",
        "\n",
        "The dataset consists of a single table (Largest_banks) with the following three key attributes:\n",
        "- **Rank:** Numerical position of the bank based on market capitalization.\n",
        "- **Bank Name:** Name of the financial institution.\n",
        "- **Market Cap (USD Billion):** Market valuation of the bank in USD billions.\n",
        "This dataset enables analysis of global banking trends, cross-currency valuations, and the relative market dominance of financial institutions.\n",
        "\n",
        "An additional data on market exchange rates is available to compare the marketcap dealing with different currencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r5GZtubCBLx"
      },
      "source": [
        "<h2>Assignment Tasks</h2>\n",
        "\n",
        "<ol>\n",
        "    <li>\n",
        "        <strong>Data Preparation</strong>\n",
        "        <p>The dataset consists of a structured table containing global banking data. Before performing any analysis, it is crucial to prepare the data to ensure consistency and efficiency in processing.</p>\n",
        "        Check for data consistency and ensure all columns are correctly formatted.<br>\n",
        "        Structure and prepare the data for further processing and analysis.<br>\n",
        "</br>\n",
        "    <li>\n",
        "        <strong>Data Cleaning</strong>\n",
        "        <ul>\n",
        "            2.1. <strong>Handling Missing Values:</strong> Decide on an approach to handle missing data (e.g., imputation or removal). Mention the approach in your report.<br>\n",
        "            2.2. <strong>Fixing Columns:</strong> Ensure all columns are properly named and formatted.<br>\n",
        "            2.3. <strong>Handling Outliers:</strong> Identify outliers in the dataset and explain why they are considered outliers. It is not necessary to remove them for this task, but mention your approach for handling them.<br>\n",
        "        </ul>\n",
        "</br>\n",
        "    <li>\n",
        "        <strong>Exploratory Data Analysis:</strong></br>\n",
        "            EDA Finding Patterns - Analyze the dataset and find patterns based on the following points:\n",
        "                <ul>\n",
        "                    3.1. Convert PySpark DataFrame to Pandas DataFrame for visualization.<br>\n",
        "                    3.2. Analyze the distribution of market capitalization using a histogram.<br>\n",
        "                    3.3. Identify the top 10 banks by market capitalization using a bar chart.<br>\n",
        "                    3.4. Visualize the relationship between market capitalization and bank ranking using a scatter plot.<br>\n",
        "                    3.5. Use a boxplot to examine the spread and outliers in market capitalization.<br>\n",
        "                    3.6. Display the quartile distribution of market capitalization using a violin plot.<br>\n",
        "                    3.7. Compute cumulative market share and visualize it with a line plot.<br>\n",
        "                    3.8. Categorize banks into market capitalization ranges and analyze their distribution using a bar chart.<br>\n",
        "                    3.9. Calculate and display market share distribution of top 10 banks using a pie chart.<br>\n",
        "                </ul>\n",
        "        </ul>\n",
        "</br>\n",
        "    <li>\n",
        "        <strong>Banking Data ETL Querying</strong>\n",
        "        <ul>\n",
        "            4.1. Perform Advanced Market Capitalization Analysis with Growth Metrics.<br>\n",
        "            4.2. Analyze Market Concentration and Categorize Banks Based on Market Share Tiers.<br>\n",
        "            4.3. Examine Statistical Distribution of Market Capitalization Using Quartile Analysis.<br>\n",
        "            4.4. Conduct Comparative Size Analysis to Classify Banks by Relative Market Size.<br>\n",
        "            4.5. Evaluate Market Growth and Identify Gaps Between Consecutive Banks.<br>\n",
        "            4.6. Assess Market Dominance by Measuring Cumulative Share and Dominance Score.<br>\n",
        "            4.7. Analyze Segment-Wise Bank Performance Based on Market Capitalization Ranges.<br>\n",
        "            4.8. Generate a Comprehensive Performance Dashboard for Bank Rankings and Metrics.<br>\n",
        "        </ul>\n",
        "    </br>\n",
        "    <li>\n",
        "        <strong>Conclusion</strong></br>\n",
        "            Provide final insights and recommendations based on the analysis:</br>\n",
        "            <ul>\n",
        "            5.1 Recommendations to track and compare market capitalisation of the top global banks toevaluate competitiveness and dominance.</br>\n",
        "            5.2 Suggestions to use cross-currency analysis (USD, GBP, EUR, INR) for consistent benchmarking\n",
        "of financial institutions across regions.</br>\n",
        "            5.3 Propose continuous monitoring of market share concentration to identify growth opportunities\n",
        "for mid-tier banks.</br>\n",
        "            5.4  Identify potential regions or banking segments for expansion by analysing gaps between tiers\n",
        "of banks and regional trends.</br>\n",
        "            </ul>\n",
        "    </br>\n",
        "    <li>\n",
        "        <strong>Visualization Integration [Optional]</strong>\n",
        "        <p>Enhance the project by incorporating a visualization component that connects the processed data stored in an S3 bucket to a business intelligence tool such as Tableau or Power BI. This involves setting up the connection between the S3 bucket and the chosen visualization tool, importing the processed dataset for analysis and visualization, creating interactive dashboards to explore key trends and insights and ensuring data updates are reflected dynamically in the visualization tool.<br>\n",
        "        </ul>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXCc0AGFRhpg"
      },
      "source": [
        "Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Y-0TKibrt0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cbb8cf1-46c4-4b39-f942-e732dcc2dd79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# # Install the required libraries\n",
        "!pip install --quiet pyspark==3.5.4 pandas==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OEMrc5vMqFg6"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "# Import PySpark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round, count, when, isnull\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ef0bla2fqZwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "5ae51133-a49a-402f-87e3-7eb0c80def8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('64bit', 'ELF')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'findspark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2822136094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mSPARK_HOME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"C:\\spark\\spark-3.5.0-bin-hadoop3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPARK_HOME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPARK_HOME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'findspark' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankingDataAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import platform\n",
        "print(platform.architecture())\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Set your Python environment\n",
        "# -------------------------------\n",
        "# Replace this path with your Anaconda Python executable\n",
        "PYTHON_PATH = r\"C:\\Users\\anam1\\anaconda4\\python.exe\"\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"] = PYTHON_PATH\n",
        "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYTHON_PATH\n",
        "\n",
        "# Optional: increase Python worker timeout for large datasets\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--conf spark.network.timeout=600s --conf spark.executor.heartbeatInterval=60s pyspark-shell\"\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Initialize findspark\n",
        "# -------------------------------\n",
        "# Replace this with your Spark installation path\n",
        "SPARK_HOME = r\"C:\\spark\\spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
        "findspark.init(SPARK_HOME)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Start Spark session\n",
        "# -------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WindowsPySparkFix\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Test with a small DataFrame\n",
        "# -------------------------------\n",
        "data = [(1, \"Alice\"), (2, \"Bob\")]\n",
        "columns = [\"id\", \"name\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "print(\"PySpark setup successful!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr_ZO_0OtGrR"
      },
      "source": [
        "#**1. Data Preparation** <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "\n",
        "The data for this project can be accessed from the wikipedia link provided above. The page features data regarding global banking data.\n",
        "\n",
        "Utilise the pandas method to read tables from the html document\n",
        "and extract the requried data consists of a structured format.\n",
        "\n",
        "Before performing any analysis, it is crucial to prepare the data to ensure consistency, and efficiency in processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ajjgOTNWqbv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db9fa37-509f-4e95-d6fb-24b9ac1bf329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.4\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "len of tables:  4\n",
            "First table:\n",
            "    Rank                                Bank name  Market cap (US$ billion)\n",
            "0     1                           JPMorgan Chase                    432.92\n",
            "1     2                          Bank of America                    231.52\n",
            "2     3  Industrial and Commercial Bank of China                    194.56\n",
            "3     4               Agricultural Bank of China                    160.68\n",
            "4     5                                HDFC Bank                    157.91\n",
            "+----+--------------------+----------+\n",
            "|Rank|           Bank_Name|Market_cap|\n",
            "+----+--------------------+----------+\n",
            "|   1|      JPMorgan Chase|    432.92|\n",
            "|   2|     Bank of America|    231.52|\n",
            "|   3|Industrial and Co...|    194.56|\n",
            "|   4|Agricultural Bank...|    160.68|\n",
            "|   5|           HDFC Bank|    157.91|\n",
            "|   6|         Wells Fargo|    155.87|\n",
            "|   7|   HSBC Holdings PLC|     148.9|\n",
            "|   8|      Morgan Stanley|    140.83|\n",
            "|   9|China Constructio...|    139.82|\n",
            "|  10|       Bank of China|    136.81|\n",
            "+----+--------------------+----------+\n",
            "\n",
            "root\n",
            " |-- Rank: long (nullable = true)\n",
            " |-- Bank_Name: string (nullable = true)\n",
            " |-- Market_cap: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize Spark session\n",
        "# this was added python version and pysprk env were not same\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TestApp\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .config(\"spark.network.timeout\", \"600s\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "\n",
        "# URL of the Wikipedia page\n",
        "URL =\"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\"\n",
        "\n",
        "# Use pandas to read all HTML tables from the page\n",
        "\n",
        "!pip install lxml\n",
        "import pandas as pd\n",
        "tables = pd.read_html(URL)\n",
        "print(\"len of tables: \" ,len(tables))\n",
        "\n",
        "# Load the correct table (Table 1)\n",
        "\n",
        "table1 = tables[1]\n",
        "\n",
        "print(\"First table:\\n\", table1.head())\n",
        "\n",
        "# Convert pandas DataFrame to PySpark DataFrame\n",
        "\n",
        "df_spark = spark.createDataFrame(table1)\n",
        "\n",
        "# Rename columns\n",
        "\n",
        "df_spark = df_spark\\\n",
        "                    .withColumnRenamed(\"Bank name\", \"Bank_Name\")\\\n",
        "                    .withColumnRenamed(\"Market cap (US$ billion)\", \"Market_Cap\")\n",
        "# Show the first few rows of the PySpark DataFrame\n",
        "\n",
        "df_spark.show()\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Configure logging\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ALL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BOF_rPDt0i1"
      },
      "source": [
        "Load the data for exchange rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oyib8RmszOM"
      },
      "outputs": [],
      "source": [
        "# Load the exchange rate data\n",
        "\n",
        "# Show the first few rows of the exchange rate data to verify\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLXeRmoYyDLO"
      },
      "source": [
        "#**2. Data Cleaning** <font color = red>[20 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KPugt-t-Iv"
      },
      "source": [
        "##**2.1 Handle Missing Values** <font color = red>[10 marks]</font> <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBKfrZXtjKFJ"
      },
      "outputs": [],
      "source": [
        "# Print the schema to check data types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvYbzstnwH_X"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Hhyi5djH1i"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScS4eIgEuZIu"
      },
      "source": [
        "##**2.2 Fixing Columns** <font color = red>[5 marks]</font> <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bss015FcjUsi"
      },
      "source": [
        "The Market cap is already in numeric so no conversion needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuU1l94gjdSd"
      },
      "outputs": [],
      "source": [
        "# Count the total number of rows\n",
        "\n",
        "# Check if there are duplicates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKZCEj_8uKrJ"
      },
      "source": [
        "##**2.2 Handle Outliers** <font color = red>[5 marks]</font> <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3V-nyPBuUl9"
      },
      "outputs": [],
      "source": [
        "# Write code for outlier analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcH-xf86enan"
      },
      "source": [
        "Saving the Cleaned Dataset into a CSV File into the S3 Bucket\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrOtsmP4j0PZ"
      },
      "source": [
        "#**3. Exploratory Data Analysis** <font color = red>[40 marks]</font> <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nUphonQlwup"
      },
      "source": [
        "##**3.1. Conversion from PySpark to Pandas DataFrame** <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "Convert PySpark DataFrame to Pandas DataFrame for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9urgjLljo9s"
      },
      "outputs": [],
      "source": [
        "# Convert PySpark DataFrame to Pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOM6DBrXlzRD"
      },
      "source": [
        "##**3.2. Market Capitalization** <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "Analyze the distribution of market capitalization using a histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0laaO8oj9V7"
      },
      "outputs": [],
      "source": [
        "# Distribution of Market Cap (US$ Billion)\n",
        "\n",
        "# Set the style for seaborn\n",
        "\n",
        "# Plot the distribution of market cap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94mCHOlIl3zR"
      },
      "source": [
        "##**3.3. Top 10 Banks** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Identify the top 10 banks by market capitalization using a bar chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kEwUTZekA0m"
      },
      "outputs": [],
      "source": [
        "# Top 10 Banks by Market Cap\n",
        "\n",
        "# Sort the DataFrame by market cap in descending order\n",
        "\n",
        "# Plot the top 10 banks by market cap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQ8h16Gl7KJ"
      },
      "source": [
        "##**3.4. Market Cap vs Bank Ranking** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Visualize the relationship between market capitalization and bank ranking using a scatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8lE0JLXkF0u"
      },
      "outputs": [],
      "source": [
        "# Market Cap vs Rank\n",
        "\n",
        "# Plot market cap vs rank\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4qniez4mG2Z"
      },
      "source": [
        "##**3.5. Market Cap Analysis** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Use a boxplot to examine the spread and outliers in market capitalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfphnZtNkQad"
      },
      "outputs": [],
      "source": [
        "# Boxplot of Market Cap\n",
        "#To show the spread and outliers in the market capitalization data.\n",
        "\n",
        "# Plot a boxplot of market cap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9nTVBJumJQq"
      },
      "source": [
        "##**3.6. Market Cap Quartile Distribution** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Display the quartile distribution of market capitalization using a violin plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYsjprb1enao"
      },
      "outputs": [],
      "source": [
        "# Market Cap Distribution by Quartile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXxxbJnpmL67"
      },
      "source": [
        "##**3.7. Cumulative Market Share Analysis** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Compute cumulative market share and visualize it with a line plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIhkOJykenao"
      },
      "outputs": [],
      "source": [
        "# Cumulative Market Share\n",
        "\n",
        "# Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXBO_UR2mPJi"
      },
      "source": [
        "##**3.8. Categorising Banks** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "Categorize banks into market capitalization ranges and analyze their distribution using a bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndBCDmF9enao"
      },
      "outputs": [],
      "source": [
        "# Market Cap Range Distribution\n",
        "# Create market cap ranges\n",
        "\n",
        "# Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Xvh8UymTx5"
      },
      "source": [
        "##**3.9. Visualise Market Share Distribution** <font color = red>[5 Marks]</font> <br>\n",
        "\n",
        "\n",
        "Calculate and display market share distribution among the top 10 banks using a pie chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wzaewd3enao"
      },
      "outputs": [],
      "source": [
        "# Top 10 Banks Market Share\n",
        "# Calculate market share percentage for top 10 banks\n",
        "\n",
        "# Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crzFY7l2m0eh"
      },
      "source": [
        "#**4. ETL and Querying** <font color = red>[45 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkIFkDCbkZsO"
      },
      "outputs": [],
      "source": [
        "#Imports & Spark Session Initialization\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, round\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankingDataAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Logging Configuration\n",
        "logfile = \"code_log.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYX8WT5Fm-8v"
      },
      "outputs": [],
      "source": [
        "#Logging Function\n",
        "\n",
        "def log_progress(msg):\n",
        "    timeformat = '%Y-%h-%d-%H:%M:%S'\n",
        "    timestamp = datetime.now().strftime(timeformat)\n",
        "    with open(logfile, 'a') as f:\n",
        "        f.write(f\"{timestamp} : {msg}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY08Tgnnk88Q"
      },
      "outputs": [],
      "source": [
        "# Import required functions\n",
        "from pyspark.sql.functions import col, avg, sum, count, desc, asc, round, lag, dense_rank, ntile, when, first, lead\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# First, let's rename your columns to match our queries (if needed)\n",
        "\n",
        "# Create window specifications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFixnsN3sXSs"
      },
      "source": [
        "##**4.1. Market Capitalization Analysis** <font color = red>[3 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7WlpUPOm44R"
      },
      "source": [
        "Q: Perform Advanced Market Capitalization Analysis with Growth Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI2mxd6qs8kO"
      },
      "outputs": [],
      "source": [
        "# Query: Advanced Market Cap Analysis with Growth Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mg1-7rAm9Jh"
      },
      "source": [
        "##4.2.**Market Concentration Analysis** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Analyze Market Concentration and Categorize Banks Based on Market Share Tiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yta_xpoTenau"
      },
      "outputs": [],
      "source": [
        "# Market Concentration Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUKRhmvYn7rx"
      },
      "source": [
        "##**4.3. Market Capitalization Distribution** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Examine Statistical Distribution of Market Capitalization Using Quartile Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1RJ5_ooenau"
      },
      "outputs": [],
      "source": [
        "# Import required functions\n",
        "from pyspark.sql.functions import (col, avg, sum, count, desc, asc, round, lag,\n",
        "                                 dense_rank, ntile, when, first, lead, min, max, lit)\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Query 3: Statistical Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yughOvrn-ER"
      },
      "source": [
        "##**4.4. Comparative Size Analysis** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Conduct Comparative Size Analysis to Classify Banks by Relative Market Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mwW3ly4enau"
      },
      "outputs": [],
      "source": [
        "# Comparative Size Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmkMVo7hoAbA"
      },
      "source": [
        "##**4.5. Market Growth Analysis** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Evaluate Market Growth and Identify Gaps Between Consecutive Banks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlHzEFfHenau"
      },
      "outputs": [],
      "source": [
        "# Growth and Gap Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCRl_rqoCv0"
      },
      "source": [
        "##**4.6. Market Dominance Analysis** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Assess Market Dominance by Measuring Cumulative Share and Dominance Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFXDO0n-enau"
      },
      "outputs": [],
      "source": [
        "# Market Dominance Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WdJLyYsoFj4"
      },
      "source": [
        "##**4.7. Segment-Wise Bank Analysis** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Analyze Segment-Wise Bank Performance Based on Market Capitalization Ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i4tfBpienau"
      },
      "outputs": [],
      "source": [
        "# Segment Performance Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GxiOAXEoI3i"
      },
      "source": [
        "##**4.8. Performance Dashboard** <font color = red>[6 marks]</font> <br>\n",
        "\n",
        "Q: Generate a Comprehensive Performance Dashboard for Bank Rankings and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3-HWVrCenau"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Performance Dashboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s21vpAthllak"
      },
      "source": [
        "#5. Visualization Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lamWO2Qrmc1g"
      },
      "outputs": [],
      "source": [
        "# Visualization Setup Section\n",
        "\n",
        "# 1. Prepare data for visualization platforms\n",
        "# 2. Generate Tableau connection instructions\n",
        "\n",
        "# 3. Generate Power BI connection instructions\n",
        "\n",
        "# 4. Execute visualization setup\n",
        "# 5. Sample Dashboard Layout (Documentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P78eC0N6IH9g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}